import{_ as a,c as r,o as i,a5 as t,j as e,ag as o,ah as s,ai as n,aj as p}from"./chunks/framework.Cpfv34D1.js";const j=JSON.parse('{"title":"Opti-Speech","description":"","frontmatter":{"public":"true","slug":"opti-speech","tags":["My Projects"],"title":"Opti-Speech","prev":false,"next":false},"headers":[],"relativePath":"garden/opti-speech/index.md","filePath":"garden/opti-speech/index.md","lastUpdated":1717647948000}'),l={name:"garden/opti-speech/index.md"},c=t('<h1 id="opti-speech" tabindex="-1">Opti-Speech <a class="header-anchor" href="#opti-speech" aria-label="Permalink to &quot;Opti-Speech&quot;">​</a></h1><blockquote><p>Tags: <a href="/garden/my-projects/">My Projects</a></p></blockquote><p>In college I continued development on the Opti-Speech project, originally built alongside the scientific paper <a href="https://www.researchgate.net/profile/Thomas-Campbell-11/publication/354182612_Opti-speech_a_real-time_3d_visual_feedback_system_for_speech_training/links/6424679ca1b72772e4360fa2/Opti-speech-a-real-time-3d-visual-feedback-system-for-speech-training.pdf" target="_blank" rel="noreferrer">Opti-speech: a real-time, 3d visual feedback system for speech training</a></p><h2 id="the-original-project" tabindex="-1">The Original Project <a class="header-anchor" href="#the-original-project" aria-label="Permalink to &quot;The Original Project&quot;">​</a></h2><p>The Optispeech project involves designing and testing a real-time tongue model that can be viewed in a transparent head while a subject talks — for the purposes of treating speech errors and teaching foreign language sounds. This work has been conducted in partnership with Vulintus and with support from the National Institutes of Health (NIH).</p><p><img src="'+o+'" alt="system-architecture-600.jpg"></p>',6),h=e("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/9uHqIRs7ZjM",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:"",style:{display:"block",margin:"auto"}},null,-1),d=e("p",null,"This video shows a talker with WAVE sensors placed on the tongue hitting a virtual target sphere located at the alveolar ridge. When an alveolar consonant is hit (e.g., /s/, /n/, /d/) the sphere changes color from red to green.",-1),m=e("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/Oz42mKvlzqI",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:"",style:{display:"block",margin:"auto"}},null,-1),u=t('<p>This video shows an American talker learning a novel sound not found in English. When the post-alveolar consonant is hit, the target sphere changes color from red to green. Here, the NDI WAVE system serves as input.</p><h2 id="my-work" tabindex="-1">My Work <a class="header-anchor" href="#my-work" aria-label="Permalink to &quot;My Work&quot;">​</a></h2><p>As the sole programmer at UT Dallas Speech Production Lab at the time, my changes involved updating to a more modern version of Unity, improving the interface, in general cleaning up tech debt so it can more easily support new features, and added support for additional EMA systems, namely the Carstens AG501.</p><p><img src="'+s+'" alt="new-interface.png"></p><p>In addition, the program now includes documentation and unit tests to improve program stability and maintainability going forward.</p><p><img src="'+n+'" alt="documentation.png"></p><p><img src="'+p+'" alt="unittests.png"></p>',7),g=[c,h,d,m,u];function _(f,b,y,w,v,k){return i(),r("div",null,g)}const P=a(l,[["render",_]]);export{j as __pageData,P as default};
